# Recursive Graded Neural Networks

## A Fractal Framework for Hierarchical Learning via Recursive Structure Theory

**Author:** Michael A. Doran Jr.  
**Date:** July 9, 2025

---

## Abstract

We present a rigorous framework for **Recursive Graded Neural Networks (RGNNs)**, extending classical graded architectures by replacing static coordinate-wise grading with **recursively generated operator hierarchies**. Each input feature, neuron, and transformation is modeled as the output of a recursive operator $S_i(n)$ from the *Recursive Shape-Structured Notation (RSSN)*. This yields a corresponding **fractal density function**:
$$D_i(n) = \lim_{j \to \infty} \frac{F_j(x_i)}{G_j} \in [0,1],$$
where $F_j(x_i)$ is the number of unique recursive feature configurations generated by $S_i$ at depth $j$, and $G_j$ is the total number of possible configurations at depth $j$. This ratio measures the structural density of the recursive operator output.

Rooted in **Recursive Structure Foundation (RSF)**, RGNNs transcend fixed vector spaces by encoding computational processes as hierarchical generative structures. We define recursive neurons, density-weighted activations, and establish a Recursive Layer Protocol (RLP) enabling architecture adaptation to recursive convergence rates. Our approximation theorems demonstrate universality in recursive Sobolev and Besov spaces, yielding convergence rates derived from fractal smoothness classes. Classical GNNs are shown to be degenerate cases where recursive depth is constant.

**Keywords:** Neural networks, recursive structures, fractal geometry, approximation theory, algebraic geometry

---

## 1. Introduction: From Static Grading to Recursive Generation

Traditional graded neural networks define inputs in $V_q^n \subset \mathbb{R}^n$, graded by a fixed tuple $q \in \mathbb{Q}_{>0}^n$ [4]. This reflects a coarse embedding of hierarchy. RGNNs replace this with **recursive operator generation**:
$$x_i := S_i(n), \quad S_i \in \mathcal{O}_\text{RSSN}$$

Each operator $S_i$ defines a recursive process yielding a structural **fractal density**:
$$D_i(n) := \lim_{j \to \infty} \frac{F_j(x_i)}{G_j},$$
where $F_j(x_i)$ counts distinct feature activations produced by $S_i$ at recursion level $j$, and $G_j$ is the total configuration capacity of the layer at depth $j$. This density governs activation scaling, weight modulation, and convergence behavior. RGNNs thus process fractal structures rather than coordinate values.

**Motivation:** Traditional neural networks struggle with hierarchical symbolic data where structural relationships matter more than numerical values. Consider algebraic invariants of genus 2 curves, where the modular forms $(J_2, J_4, J_6, J_{10})$ exhibit deep recursive relationships that standard architectures cannot capture effectively. RGNNs address this by encoding the recursive structure of mathematical objects directly into the network architecture.

---

## 2. Operator Class Taxonomy and Recursive Foundations

### 2.1 Abstract Operator Class Definition

We formally define the abstract operator class $\mathcal{O}_{\text{RSSN}}$ as a structured hierarchy:

**Definition 4 (Abstract Recursive Operator):**
An operator $S \in \mathcal{O}_{\text{RSSN}}$ is a function $S: \mathbb{N} \to \mathbb{R}$ equipped with:
- A *generation function* $G_S: \mathbb{N} \times \mathbb{N} \to \mathbb{N}$ 
- A *configuration space function* $C_S: \mathbb{N} \to \mathbb{N}$
- A *recursive depth bound* $d_S \in \mathbb{N} \cup \{\infty\}$

### 2.2 Operator Taxonomy

**Primitive Operators** (Base level recursion):
- **Triangle**: $\text{Triangle}(n) = n^n$, $d_{\text{Triangle}} = 1$
- **Square**: $\text{Square}(n) = \text{Triangle}^n(n)$, $d_{\text{Square}} = 2$ 
- **Circle**: $\text{Circle}(n) = \text{Square}^n(n)$, $d_{\text{Circle}} = 3$

**Compositional Operators** (Structural combinations):
- **RecursiveXOR**: $A \oplus_{\text{rec}} B := \text{Square}(|A - B|)$
- **RecursiveConcat**: $A \otimes_{\text{rec}} B := \text{Circle}(A \cdot B + A + B)$
- **RecursiveMod**: $A \bmod_{\text{rec}} B := \text{Triangle}(A \bmod (B + 1))$

**Structural Combinators** (Meta-operations):
- **Density Fusion**: $\text{Fuse}_D(S_1, S_2)(n) := S_1^{D_1(n)}(S_2^{D_2(n)}(n))$
- **Recursive Composition**: $S_1 \circ_{\text{rec}} S_2 := \text{limit-preserving composition}$

### 2.3 RSF Foundations

We ground this taxonomy in the Recursive Structure Foundation (RSF) [1]:

**RSF Principle 1 (Structural Identity):**
$A = B \iff \forall n: F_n(A) = F_n(B)$

**RSF Principle 2 (Recursive Pairing):**
$\forall A, B: \exists C: F_1(C) = A \wedge F_2(C) = B$

**RSF Principle 3 (Recursive Aggregation):**
$\forall \{R_i\}, \exists R: D(R) = \sum_i \frac{D(R_i)}{G_n}$

**RSF Principle 4 (Compositional Closure):**
$\forall S_1, S_2 \in \mathcal{O}_{\text{RSSN}}: S_1 \circ_{\text{rec}} S_2 \in \mathcal{O}_{\text{RSSN}}$

This taxonomy ensures that recursive structures can be systematically combined, compared, and aggregated within the neural network framework.

---

## 3. Mathematical Framework

### Definition 1 (Effective Network Depth):
For an RGNN with recursive operators $\{S_i\}$, the effective network depth is:
$$d = \max_i \left\{ \min\{j : |D_i(j+1) - D_i(j)| < \epsilon\} \right\}$$
where $\epsilon > 0$ is a convergence tolerance. This captures the depth at which recursive processes stabilize.

### Definition 2 (Recursive Structural Measure):
The structural measure $\mu_{\text{rec}}$ on the recursive manifold is defined by:
$d\mu_{\text{rec}}(x) = \prod_i D_i(n_i)^{1/d} \, dx_i$
This measure weights regions according to their recursive density, providing the foundation for our approximation theorems.

### Definition 3 (Recursive Manifold):
A **recursive manifold** $\mathcal{M}_{\text{rec}}$ is a topological space where:
- Points correspond to recursive structures $(S, D)$
- Open sets are generated by operator-defined neighborhoods: $U_S(\epsilon) = \{T : |D_S - D_T| < \epsilon\}$
- The manifold structure is induced by the recursive density metric

**Recursive Metric Structure:**
We define a metric $d_{\text{rec}}(x, y)$ on $\mathcal{M}_{\text{rec}}$ based on divergence in recursive density sequences:
$d_{\text{rec}}(x, y) = \sqrt{\sum_{k=1}^{\infty} \frac{1}{2^k} (D_k(x) - D_k(y))^2}$

This metric satisfies the triangle inequality and enables geometric analysis of recursive structures.

**Riemannian Structure:**
The recursive manifold carries a natural Riemannian metric:
$g_{ij}^{\text{rec}} = \frac{\partial^2}{\partial D_i \partial D_j} \sum_k D_k \log D_k$

This induces geodesics corresponding to optimal recursive transformations and enables differential geometric analysis of RGNN architectures.

### Definition 4 (Recursive Smoothness Classes):
We define recursive Sobolev spaces $\mathcal{W}_{\text{rec}}^{(k)}$ and Besov spaces $\mathcal{B}_{\text{rec}}^{(s)}_{p,r}$ where functions have derivatives up to order $k$ (or smoothness $s$) in the recursive density metric.

### Lemma 1 (Density Continuity):
If $S_i \in \mathcal{O}_\text{fractal}$, then $D_i(n)$ exists and is continuous on $\mathbb{N}$.

**Proof:** The continuity follows from the uniform convergence of the sequence $\{F_j(x_i)/G_j\}_{j=1}^{\infty}$ when $S_i$ generates fractal structures with bounded complexity growth.

### Lemma 2 (Recursive Convergence Stability):
If $|D_i(n+1) - D_i(n)| \leq C/n^k$ for some constants $C, k > 0$, then $x_i = S_i(n) \in \mathcal{W}_{\text{rec}}^{(k)}$.

**Proof:** The Cauchy criterion ensures convergence, and the power-law decay rate determines the recursive smoothness class.

### Theorem 1 (Recursive Sobolev Approximation):
If $f \in \mathcal{W}_{\text{rec}}^{(k)}$, there exists a network $\Phi_m$ such that:
$$\|D_f - D_{\Phi_m}\|_{L^2(\mu_{\text{rec}})} \leq C m^{-k/d},$$
where $d$ is the effective network depth and $C$ is independent of $m$.

**Proof:** We construct $\Phi_m$ by discretizing the recursive structure at scale $1/m$ and show that the approximation error decays as the inverse of the discretization parameter raised to the power of the smoothness index.

### Theorem 2 (Recursive Besov Approximation):
If $f \in \mathcal{B}_{\text{rec}}^{(s)}_{p,r}$, then:
$$\|D_f - D_{\Phi_m}\|_{L^p(\mu_{\text{rec}})} = \mathcal{O}(m^{-s/d})$$

### Theorem 3 (Classical Recovery):
A classical GNN with grading $q_i \in \mathbb{Q}$ is equivalent to an RGNN with constant densities $D_i(n) = 1/q_i$.

**Proof:** When recursive depth is constant, the fractal density stabilizes at $1/q_i$, reducing to the classical graded case.

### Theorem 4 (Recursive Universality):
$$\forall f \in \mathcal{W}_{\text{rec}}^{(k)} \cup \mathcal{B}_{\text{rec}}^{(s)}_{p,r}, \exists \{\Phi_m\} \text{ such that } D_{\Phi_m} \rightarrow D_f \text{ as } m \rightarrow \infty$$

---

## 4. Recursive Layer Protocol (RLP)

### Forward Pass Architecture:
* **Input generation**: $x_i = S_i(n)$, where $S_i \in \{\text{Triangle}, \text{Square}, \text{Circle}, \ldots\}$
* **Density computation**: $D_i(n) = \lim_{j \to \infty} \frac{F_j(x_i)}{G_j}$ (approximated computationally)
* **Weight modulation**: $w_i := \rho(D_i(n))$, where $\rho: [0,1] \to \mathbb{R}$ is a learned scaling function
* **Density-weighted activation**: $\sigma_D(x) = \max(0, |x|^{1/D})$ or $\sigma_D(x) = \frac{e^{x/D} - 1}{D}$
* **Layer output**: $y_j = \sum_i w_i \cdot \sigma_{D_i}(x_i) + b_j$

### Training Algorithm:

**Enhanced Differentiable Approximation:** We employ entropy-regularized approximations that connect to modern generative modeling:

**Method 1 (Softmax-Based Approximation):**
$\tilde{F}_j(x_i) = G_j \sum_{k=1}^{G_j} \text{softmax}_\beta(f_k(x_i))_k$
where $\text{softmax}_\beta(z)_k = \frac{e^{\beta z_k}}{\sum_l e^{\beta z_l}}$ with temperature parameter $\beta$.

**Method 2 (Score-Based Approximation):**
Following score-based generative models, we treat recursive density as a score function:
$\tilde{F}_j(x_i) = G_j \cdot \sigma\left(\beta \cdot \nabla_{x_i} \log p_j(x_i)\right)$
where $p_j(x_i)$ is the empirical distribution of configurations at level $j$.

**Method 3 (Diffusion-Inspired Recursion):**
We model recursive generation as a forward diffusion process:
$F_{j+1}(x_i) = F_j(x_i) + \sqrt{\alpha_j} \epsilon_j + \beta_j \nabla_{x_i} \log F_j(x_i)$
where $\epsilon_j \sim \mathcal{N}(0, I)$ and $\alpha_j, \beta_j$ are learned diffusion parameters.

**Gradient computation** (unified framework):
$\nabla_{\theta} D_i(n) = \frac{1}{G_j} \nabla_{\theta} \tilde{F}_j(x_i)$

**Enhanced Loss Function:**
$\mathcal{L} = \mathcal{L}_{\text{task}} + \alpha \mathcal{L}_{\text{struct}} + \gamma \mathcal{L}_{\text{smooth}} + \delta \mathcal{L}_{\text{entropy}}$
where:
$\mathcal{L}_{\text{struct}} = \sum_i (D_i^{\text{target}} - D_i^{\text{pred}})^2$
$\mathcal{L}_{\text{smooth}} = \sum_i |D_{i+1} - D_i|^2$
$\mathcal{L}_{\text{entropy}} = -\sum_i D_i \log D_i - (1-D_i) \log(1-D_i)$

The entropy regularization encourages diverse recursive patterns while preventing mode collapse in the density space.

---

## 5. Recursive Structure Embedding

Each layer is characterized by its **recursive signature**:
$$\mathcal{L}^{(d)} := \{(S_i^{(d)}, D_i^{(d)}) \}_{i=1}^{n_d}$$

The **recursive complexity** of the network is:
$$\mathcal{C}_{\text{rec}} = \sum_{d=1}^{L} \sum_{i=1}^{n_d} H(D_i^{(d)})$$
where $H(p) = -p \log p - (1-p) \log(1-p)$ is the binary entropy function.

This complexity measure captures the information-theoretic content of the recursive structure and can be used for regularization or architecture selection.

---

## 6. Geometric and Topological Properties

### 6.1 Recursive Manifold Geometry

The recursive manifold $\mathcal{M}_{\text{rec}}$ exhibits rich geometric structure that informs RGNN design:

**Theorem 5 (Recursive Curvature):**
The scalar curvature of $\mathcal{M}_{\text{rec}}$ at point $(S, D)$ is:
$R_{\text{rec}}(S, D) = \sum_{i,j} \frac{\partial^2 D_i}{\partial S_j^2} - \sum_i (D_i - \bar{D})^2$
where $\bar{D} = \frac{1}{n}\sum_i D_i$ is the mean density.

High curvature regions correspond to rapid changes in recursive structure, indicating critical transition points in the learning dynamics.

**Corollary 5.1 (Geodesic Learning):**
Optimal parameter updates in RGNNs follow geodesics on $\mathcal{M}_{\text{rec}}$:
$\frac{d^2 \theta}{dt^2} + \Gamma_{ij}^k \frac{d\theta^i}{dt} \frac{d\theta^j}{dt} = 0$
where $\Gamma_{ij}^k$ are the Christoffel symbols of the recursive metric.

### 6.2 Topological Invariants

**Definition 5 (Recursive Homology):**
We define homology groups $H_k(\mathcal{M}_{\text{rec}}, \mathbb{Z})$ where:
- $H_0$ counts connected components (distinct recursive families)
- $H_1$ captures cycles in recursive operator composition
- Higher $H_k$ reveal topological obstructions to universal approximation

**Theorem 6 (Topological Universality):**
If $H_k(\mathcal{M}_{\text{rec}}, \mathbb{Z}) = 0$ for $k \geq 2$, then RGNNs achieve universal approximation in recursive smoothness classes.

### 6.3 Connections to Generative Models

The diffusion-inspired recursive generation creates natural connections to modern generative modeling:

**Recursive Score Matching:**
The recursive density $D_i(n)$ can be interpreted as a score function $\nabla \log p_i(x)$ where $p_i$ is the distribution of recursive structures at level $i$.

**Denoising Recursive Autoencoders:**
RGNNs naturally implement a denoising process where:
1. Forward pass: Add noise to recursive structure
2. Reverse pass: Recover clean recursive patterns
3. Training: Minimize reconstruction error in density space

This connection enables transfer learning from pre-trained diffusion models to recursive structure learning tasks.

---

## 7. Worked Example: XOR Function

Consider the XOR function with inputs $x_1, x_2 \in \{0,1\}$.

**Classical approach:** 2-2-1 network with ReLU activations.

**RGNN approach:**
* **Input operators:** $S_1(n) = \text{Triangle}(x_1, n)$, $S_2(n) = \text{Triangle}(x_2, n)$
* **Recursive densities:** $D_1(n) \approx 1/n$, $D_2(n) \approx 1/n$ (sparse structure for single bits)
* **Hidden combination:** $S_3(n) = \text{Square}(S_1 \oplus_{\text{rec}} S_2, n)$ where $\oplus_{\text{rec}}$ is recursive XOR
* **Output density:** $D_3(n) \approx 1/2^n$ (exponentially sparse, capturing XOR exclusivity)

The recursive structure naturally encodes logical exclusivity through density patterns, providing interpretable representations of the learned function.

**Computational Details:**
For inputs $(0,1)$:
1. $S_1(2) = \text{Triangle}(0,2) = 0$, $D_1 = 0$
2. $S_2(2) = \text{Triangle}(1,2) = 1$, $D_2 = 1/2$
3. $S_3(2) = \text{Square}(1,2) = \text{Triangle}^2(1) = 1$, $D_3 = 1/4$
4. Output: $\sigma_{1/4}(1) = 1^4 = 1$ ✓

---

## 7. Applications to Algebraic Geometry

**Genus 2 Modular Invariants:** Consider the classical problem of computing modular invariants $(J_2, J_4, J_6, J_{10})$ for genus 2 curves. These invariants exhibit deep recursive relationships that traditional neural networks struggle to capture.

**RGNN Architecture for Modular Forms:**
* **Input layer:** Coefficients of genus 2 curves encoded as recursive operators
* **Hidden layers:** Capture the recursive structure of modular transformations
* **Output layer:** Predicts invariant values through density-weighted combinations

**Advantages over classical approaches:**
1. **Structural preservation:** Recursive densities maintain the hierarchical relationships between modular forms
2. **Interpretability:** Density patterns reveal the underlying algebraic structure
3. **Generalization:** Framework extends naturally to higher genus curves

---

## 8. Computational Complexity and Implementation

**Complexity Analysis:**
* **Forward pass:** $O(n \cdot d \cdot \log d)$ where $n$ is network width and $d$ is effective recursive depth
* **Backward pass:** $O(n \cdot d^2)$ due to recursive gradient computation
* **Memory:** $O(n \cdot d)$ for storing recursive signatures

**Implementation Considerations:**
```python
class RecursiveLayer(nn.Module):
    def __init__(self, input_dim, output_dim, max_depth=10):
        super().__init__()
        self.operators = nn.ModuleList([
            RecursiveOperator(op_type) for op_type in ['triangle', 'square', 'circle']
        ])
        self.density_net = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.ReLU(),
            nn.Linear(64, output_dim),
            nn.Sigmoid()  # Ensure density ∈ [0,1]
        )
        self.max_depth = max_depth
    
    def forward(self, x):
        densities = self.density_net(x)
        recursive_features = []
        
        for i, op in enumerate(self.operators):
            feat = op(x, depth=min(self.max_depth, int(10 * densities[i])))
            recursive_features.append(feat)
        
        return torch.stack(recursive_features, dim=1)
```

---

## 9. Future Work and Extensions

### 9.1 Theoretical Extensions
* **Higher-order recursive structures:** Extension to hypergeometric and elliptic recursive patterns
* **Quantum recursive networks:** Integration with quantum computing frameworks
* **Category-theoretic foundations:** Formalization using topos theory and higher categories

### 9.2 Applications
* **Mathematical discovery:** Automated theorem proving using recursive pattern recognition
* **Scientific modeling:** Complex systems with hierarchical recursive dynamics
* **Cryptography:** Recursive structures for post-quantum cryptographic protocols

### 9.3 Computational Improvements
* **Parallel recursive computation:** GPU optimization for large-scale recursive operations
* **Approximate density computation:** Fast approximation algorithms for real-time applications
* **Hardware acceleration:** Neuromorphic chips optimized for recursive computation

---

## References

[1] Doran, M. A. Jr. *Recursive Structure Foundation*, 2025.
[2] Doran, M. A. Jr. *Recursive Shape-Structured Notation*, 2025.
[3] Doran, M. A. Jr. *Fractal Tensor Calculus*, 2025.
[4] Shaska, T. *Graded Neural Networks*, arXiv:2502.17751v2, 2025.
[5] Boyd, S. & Vandenberghe, L. *Convex Optimization*, Cambridge University Press, 2004.
[6] DeVore, R. A. *Nonlinear Approximation*, Acta Numerica, 1998.
[7] Hornik, K. *Approximation Capabilities of Multilayer Feedforward Networks*, Neural Networks, 1991.
[8] Cohen, H. & Frey, G. *Handbook of Elliptic and Hyperelliptic Curve Cryptography*, CRC Press, 2005.
[9] Dolgachev, I. *Classical Algebraic Geometry: A Modern View*, Cambridge University Press, 2012.

---

## Conclusion

Recursive Graded Neural Networks provide a mathematically rigorous framework for hierarchical learning through recursive structure theory. By replacing static grading with dynamic recursive generation, RGNNs capture complex structural patterns while maintaining theoretical guarantees through approximation theorems in recursive smoothness classes.

The framework naturally incorporates density-based learning, differentiable recursive computation, and connections to classical approximation theory. Applications to algebraic geometry demonstrate the practical utility of recursive structural representations, while the theoretical foundations position RGNNs as a principled extension of traditional neural architectures for symbolic and hierarchical data.

Future work will focus on computational optimization, extended applications to mathematical discovery, and deeper theoretical connections to category theory and quantum information. The recursive paradigm opens new possibilities for understanding and modeling the hierarchical structures that pervade mathematics, science, and nature.
